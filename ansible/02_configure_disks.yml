---

- hosts: all
  tasks:
    - name: Add an Apt signing key, uses whichever key is at the URL
      apt_key:
        url: https://nvidia.github.io/libnvidia-container/gpgkey
        state: present
        keyring: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    - name: Create list file
      copy:
        dest: "/etc/apt/sources.list.d/nvidia-container-toolkit.list"
        content: "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/$(ARCH) / \n"
        group: root
        owner: root
        mode: 0644
    - name: Install nvidia-container-toolkit-base
      apt:
        name: nvidia-container-toolkit-base=1.14.2-1
        state: present
        update_cache: yes
    - name: Install nvidia-container-toolkit
      apt:
        name: nvidia-container-toolkit=1.14.2-1
        state: present
    - name: Install linux headers
      apt:
        name: "linux-headers-{{ ansible_kernel }}"
        state: present
 #   - name: Install cuda drivers keyring
 #     apt:
 #       deb: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
 #       state: present
 #   - name: Install cuda drivers
 #     apt:
 #       name: cuda-drivers=1.14.2-1
 #       state: present
 #       update_cache: yes
    - name: Copy containerd config
      copy:
        src: /ssm/external/ansible/containerd.config
        dest: /etc/containerd/config.toml
        force: true
        mode: preserve
      notify:
        - restart containerd
    - name: create partition for local cache
      parted:
        device: "{{ local_cache_disk.device }}"
        number: "{{ local_cache_disk.number | default(1) }}"
        flags: [ lvm ]
        state: present
        part_end: "{{ local_cache_disk.part_end }}"
      register: local_cache_partition
    - name: create partitions for distributed cache
      parted:
        device: "{{ item.device }}"
        number: "{{ item.number }}"
        flags: [ lvm ]
        state: present
        part_start: "{{ item.part_start | default('0%')}}"
        part_end: "{{ item.part_end | default('100%')}}"
      loop: "{{ distributed_cache_disks }}"
      register: distributed_cache_partitions
    - name: merge distributed cache partitions into a single list SSD 
      set_fact:
        distrib_partitions_list: |-
          [
          {% for partition in distributed_cache_partitions.results %}
            "{{ partition.item.device}}{{ partition.item.number }}",
          {% endfor %}
          ]
      when: '"/dev/sd" in local_cache_disk.device'
    - name: merge distributed cache partitions into a single list NVME 
      set_fact:
        distrib_partitions_list: |-
          [
          {% for partition in distributed_cache_partitions.results %}
            "{{ partition.item.device}}p{{ partition.item.number }}",
          {% endfor %}
          ]
      when: '"nvme" in local_cache_disk.device'
    - name: Display local cache partitions
      debug:
        var: local_cache_partition
    - name: Display distributed cache partitions
      debug:
        var: distrib_partitions_list
    - name: create volume group for local cache SSD
      lvg:
        vg: csi-lvm-local-cache
        pvs: "{{ local_cache_partition.disk.dev }}{{ local_cache_partition.partitions[0].num }}"
      when: '"/dev/sd" in local_cache_disk.device'
    - name: create volume group for local cache NVME
      lvg:
        vg: csi-lvm-local-cache
        pvs: "{{ local_cache_partition.disk.dev }}p{{ local_cache_partition.partitions[0].num }}"
      when: '"nvme" in local_cache_disk.device'
    - name: create volume group for distributed cache
      lvg:
        vg: csi-lvm-distributed-cache
        pvs: "{{ distrib_partitions_list }}"
        state: present
    - name: Install lvm2 dependency
      package:
        name: lvm2
        state: present
  handlers:
    - name: restart containerd
      service:
        name=containerd
        state=restarted